# 2-EKS - Amazon EKS Cluster

## ğŸ“‹ DescriÃ§Ã£o

Esta camada cria o cluster Amazon EKS (Elastic Kubernetes Service) com node groups e todas as configuraÃ§Ãµes IAM necessÃ¡rias.

## ğŸ¯ Recursos Criados

### EKS Cluster
- **Nome**: eks-soat-fast-food-dev
- **VersÃ£o**: 1.32
- **Endpoint**: PÃºblico habilitado, privado desabilitado
- **Authentication Mode**: API
- **Pod Identity Addon**: v1.3.8-eksbuild.2
- **Control Plane Logs**: api, audit, authenticator, controllerManager, scheduler

### IAM Roles
- **Cluster Role**: PermissÃµes para o control plane do EKS
- **Node Group Role**: PermissÃµes para os worker nodes
  - AmazonEKSWorkerNodePolicy
  - AmazonEKS_CNI_Policy
  - AmazonEC2ContainerRegistryReadOnly

### Node Group
- **Tipo**: ON_DEMAND
- **InstÃ¢ncias**: t3.micro
- **Scaling**:
  - Desired: 1
  - Min: 1
  - Max: 2
- **Subnets**: Privadas (da camada networking)

## âš™ï¸ ConfiguraÃ§Ã£o

### Backend

Esta camada usa S3 como backend remoto:
- **Bucket**: soat-fast-food-terraform-states
- **Key**: 2-eks/terraform.tfstate
- **Region**: us-east-1
- **Encryption**: Habilitada

### DependÃªncias

Esta camada depende da camada **1-networking** via `terraform_remote_state`:
- Busca subnet IDs das subnets privadas
- Usa VPC criada na camada anterior

### VariÃ¡veis

| VariÃ¡vel | DescriÃ§Ã£o | Valor PadrÃ£o |
|----------|-----------|--------------|
| `aws_region` | RegiÃ£o AWS | `us-east-1` |
| `aws_profile` | Perfil AWS CLI | `default` |
| `project_name` | Nome do projeto | `soat-fast-food` |
| `environment` | Ambiente | `dev` |
| `cluster_name` | Nome do cluster EKS | `eks-soat-fast-food-dev` |
| `cluster_version` | VersÃ£o do Kubernetes | `1.32` |
| `cluster_log_types` | Logs do control plane habilitados | `["api","audit","authenticator","controllerManager","scheduler"]` |
| `endpoint_private_access` | Habilitar endpoint privado | `false` |
| `endpoint_public_access` | Habilitar endpoint pÃºblico | `true` |
| `public_access_cidrs` | CIDRs permitidos no endpoint pÃºblico | `["0.0.0.0/0"]` |
| `pod_identity_addon_version` | VersÃ£o do Pod Identity addon | `v1.3.8-eksbuild.2` |
| `node_group_capacity_type` | Tipo de capacidade | `ON_DEMAND` |
| `node_group_instance_types` | Tipos de instÃ¢ncia | `["t3.micro"]` |
| `node_group_desired_size` | Tamanho desejado | `1` |
| `node_group_min_size` | Tamanho mÃ­nimo | `1` |
| `node_group_max_size` | Tamanho mÃ¡ximo | `2` |

### Outputs

| Output | DescriÃ§Ã£o |
|--------|-----------|
| `cluster_id` | ID do cluster EKS |
| `cluster_name` | Nome do cluster EKS |
| `cluster_endpoint` | Endpoint da API do cluster |
| `cluster_version` | VersÃ£o do Kubernetes |
| `cluster_certificate_authority` | Certificado CA do cluster (sensÃ­vel) |
| `cluster_security_group_id` | ID do security group do cluster |
| `cluster_iam_role_arn` | ARN da role IAM do cluster |
| `node_group_id` | ID do node group |
| `node_group_arn` | ARN do node group |
| `node_group_status` | Status do node group |
| `node_group_iam_role_arn` | ARN da role IAM do node group |
| `oidc_provider_arn` | URL do issuer OIDC do cluster |

## ğŸš€ Como Usar

### PrÃ©-requisitos

1. Camada 0-bootstrap aplicada
2. Camada 1-networking aplicada
3. AWS CLI configurado com perfil `default`

### 1. Inicializar Terraform

```bash
cd terraform/2-eks
terraform init
```

### 2. Planejar

```bash
terraform plan
```

### 3. Aplicar

```bash
terraform apply
```

### 4. Configurar kubectl

ApÃ³s a criaÃ§Ã£o do cluster, configure o kubectl:

```bash
aws eks update-kubeconfig --name eks-soat-fast-food-dev --region us-east-1 --profile default
```

### 5. Verificar Cluster

```bash
kubectl get nodes
kubectl get pods -A
```

## ğŸ“Š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EKS Control Plane                      â”‚
â”‚         (Gerenciado pela AWS)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ API Server
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Node Group                         â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Worker     â”‚              â”‚   Worker     â”‚   â”‚
â”‚  â”‚   Node 1     â”‚              â”‚   Node 2     â”‚   â”‚
â”‚  â”‚   t3.micro   â”‚              â”‚   t3.micro   â”‚   â”‚
â”‚  â”‚              â”‚              â”‚              â”‚   â”‚
â”‚  â”‚ Private      â”‚              â”‚ Private      â”‚   â”‚
â”‚  â”‚ Subnet AZ1   â”‚              â”‚ Subnet AZ2   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ DependÃªncias

### Depende de:
- âœ… 0-bootstrap (bucket S3 para state)
- âœ… 1-networking (VPC e subnets)

### Ã‰ usado por:
- â­ï¸ 3-kubernetes (Helm charts e addons)

## âš ï¸ Importante

- O cluster leva ~10-15 minutos para ser criado
- Os nodes levam ~5 minutos adicionais
- O endpoint pÃºblico estÃ¡ habilitado para facilitar acesso
- Pod Identity addon Ã© necessÃ¡rio para IAM roles for service accounts
- O node group tem `ignore_changes` no `desired_size` para evitar conflitos com autoscaling

## ğŸ” SeguranÃ§a

- Cluster usa IAM roles com least privilege
- Nodes estÃ£o em subnets privadas
- Acesso Ã  internet via NAT Gateway
- Pod Identity habilitado para workloads
- Bootstrap cluster creator admin permissions habilitado
- Endpoint pÃºblico pode ser restringido via `public_access_cidrs` (padrÃ£o aberto)

## ğŸ’° Custos

Principais componentes de custo:
- **EKS Control Plane**: ~$0.10/hora (~$73/mÃªs)
- **EC2 Instances (t3.micro)**: custo on-demand baixo (elegÃ­vel ao Free Tier)
- **NAT Gateway**: ~$0.045/hora + data transfer
- **EBS Volumes**: IncluÃ­dos com nodes

## ğŸ—‘ï¸ DestruiÃ§Ã£o

Para destruir esta camada, **primeiro destrua as camadas dependentes**:

```bash
# Destruir camadas dependentes primeiro
cd ../3-kubernetes && terraform destroy

# EntÃ£o destruir EKS
cd ../2-eks && terraform destroy
```

## ğŸ”§ Troubleshooting

### Cluster nÃ£o cria
- Verifique se a camada networking estÃ¡ aplicada
- Confirme que as subnets privadas existem
- Verifique permissÃµes IAM

### Nodes nÃ£o aparecem
- Aguarde 5-10 minutos apÃ³s criaÃ§Ã£o do cluster
- Verifique logs do node group no console AWS
- Confirme que NAT Gateway estÃ¡ funcionando

### kubectl nÃ£o conecta
- Execute `aws eks update-kubeconfig` novamente
- Verifique credenciais AWS
- Confirme que endpoint pÃºblico estÃ¡ habilitado

## ğŸ“ Notas

- O cluster usa authentication mode "API" para compatibilidade
- Bootstrap cluster creator default permissions facilita acesso inicial
- Node group usa lifecycle ignore_changes para desired_size
- Pod Identity addon Ã© necessÃ¡rio para prÃ³xima camada (kubernetes)
